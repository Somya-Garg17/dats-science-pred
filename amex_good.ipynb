{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-05T17:15:04.118668Z",
     "iopub.status.busy": "2025-07-05T17:15:04.118272Z",
     "iopub.status.idle": "2025-07-05T17:15:15.513485Z",
     "shell.execute_reply": "2025-07-05T17:15:15.512480Z",
     "shell.execute_reply.started": "2025-07-05T17:15:04.118611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:15:15.515796Z",
     "iopub.status.busy": "2025-07-05T17:15:15.515196Z",
     "iopub.status.idle": "2025-07-05T17:16:10.175280Z",
     "shell.execute_reply": "2025-07-05T17:16:10.172384Z",
     "shell.execute_reply.started": "2025-07-05T17:15:15.515771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "train = pd.read_parquet(\"data/train_data.parquet\")\n",
    "events = pd.read_parquet(\"data/add_event.parquet\")\n",
    "trans = pd.read_parquet(\"data/add_trans.parquet\")\n",
    "offers = pd.read_parquet(\"data/offer_metadata.parquet\")\n",
    "test = pd.read_parquet(\"data/test_data.parquet\")\n",
    "dict_df = pd.read_csv(\"data/data_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770164 entries, 0 to 770163\n",
      "Data columns (total 372 columns):\n",
      " #    Column  Non-Null Count   Dtype \n",
      "---   ------  --------------   ----- \n",
      " 0    id1     770164 non-null  object\n",
      " 1    id2     770164 non-null  object\n",
      " 2    id3     770164 non-null  object\n",
      " 3    id4     770164 non-null  object\n",
      " 4    id5     770164 non-null  object\n",
      " 5    y       770164 non-null  object\n",
      " 6    f1      278506 non-null  object\n",
      " 7    f2      322972 non-null  object\n",
      " 8    f3      108562 non-null  object\n",
      " 9    f4      68869 non-null   object\n",
      " 10   f5      538354 non-null  object\n",
      " 11   f6      620055 non-null  object\n",
      " 12   f7      402726 non-null  object\n",
      " 13   f8      535649 non-null  object\n",
      " 14   f9      485906 non-null  object\n",
      " 15   f10     516499 non-null  object\n",
      " 16   f11     413602 non-null  object\n",
      " 17   f12     527792 non-null  object\n",
      " 18   f13     696 non-null     object\n",
      " 19   f14     696 non-null     object\n",
      " 20   f15     696 non-null     object\n",
      " 21   f16     696 non-null     object\n",
      " 22   f17     696 non-null     object\n",
      " 23   f18     696 non-null     object\n",
      " 24   f19     696 non-null     object\n",
      " 25   f20     696 non-null     object\n",
      " 26   f21     696 non-null     object\n",
      " 27   f22     479840 non-null  object\n",
      " 28   f23     479840 non-null  object\n",
      " 29   f24     479840 non-null  object\n",
      " 30   f25     479840 non-null  object\n",
      " 31   f26     479840 non-null  object\n",
      " 32   f27     479840 non-null  object\n",
      " 33   f28     731513 non-null  object\n",
      " 34   f29     224295 non-null  object\n",
      " 35   f30     695141 non-null  object\n",
      " 36   f31     497548 non-null  object\n",
      " 37   f32     506790 non-null  object\n",
      " 38   f33     28467 non-null   object\n",
      " 39   f34     404 non-null     object\n",
      " 40   f35     210889 non-null  object\n",
      " 41   f36     38816 non-null   object\n",
      " 42   f37     1245 non-null    object\n",
      " 43   f38     753271 non-null  object\n",
      " 44   f39     539443 non-null  object\n",
      " 45   f40     205605 non-null  object\n",
      " 46   f41     586765 non-null  object\n",
      " 47   f42     450736 non-null  object\n",
      " 48   f43     450736 non-null  object\n",
      " 49   f44     579501 non-null  object\n",
      " 50   f45     579501 non-null  object\n",
      " 51   f46     579501 non-null  object\n",
      " 52   f47     579501 non-null  object\n",
      " 53   f48     266453 non-null  object\n",
      " 54   f49     579501 non-null  object\n",
      " 55   f50     579501 non-null  object\n",
      " 56   f51     579501 non-null  object\n",
      " 57   f52     579501 non-null  object\n",
      " 58   f53     515697 non-null  object\n",
      " 59   f54     515697 non-null  object\n",
      " 60   f55     515697 non-null  object\n",
      " 61   f56     515697 non-null  object\n",
      " 62   f57     337272 non-null  object\n",
      " 63   f58     515697 non-null  object\n",
      " 64   f59     761961 non-null  object\n",
      " 65   f60     761977 non-null  object\n",
      " 66   f61     761979 non-null  object\n",
      " 67   f62     761979 non-null  object\n",
      " 68   f63     761979 non-null  object\n",
      " 69   f64     19947 non-null   object\n",
      " 70   f65     761979 non-null  object\n",
      " 71   f66     19950 non-null   object\n",
      " 72   f67     761971 non-null  object\n",
      " 73   f68     761541 non-null  object\n",
      " 74   f69     761928 non-null  object\n",
      " 75   f70     19950 non-null   object\n",
      " 76   f71     761979 non-null  object\n",
      " 77   f72     761979 non-null  object\n",
      " 78   f73     761968 non-null  object\n",
      " 79   f74     761979 non-null  object\n",
      " 80   f75     761979 non-null  object\n",
      " 81   f76     761603 non-null  object\n",
      " 82   f77     761979 non-null  object\n",
      " 83   f78     428831 non-null  object\n",
      " 84   f79     33772 non-null   object\n",
      " 85   f80     78 non-null      object\n",
      " 86   f81     65526 non-null   object\n",
      " 87   f82     250586 non-null  object\n",
      " 88   f83     350750 non-null  object\n",
      " 89   f84     886 non-null     object\n",
      " 90   f85     734909 non-null  object\n",
      " 91   f86     761979 non-null  object\n",
      " 92   f87     761979 non-null  object\n",
      " 93   f88     19950 non-null   object\n",
      " 94   f89     761979 non-null  object\n",
      " 95   f90     761979 non-null  object\n",
      " 96   f91     761979 non-null  object\n",
      " 97   f92     19950 non-null   object\n",
      " 98   f93     761979 non-null  object\n",
      " 99   f94     762461 non-null  object\n",
      " 100  f95     762461 non-null  object\n",
      " 101  f96     762461 non-null  object\n",
      " 102  f97     762461 non-null  object\n",
      " 103  f98     762461 non-null  object\n",
      " 104  f99     762461 non-null  object\n",
      " 105  f100    762461 non-null  object\n",
      " 106  f101    762461 non-null  object\n",
      " 107  f102    762461 non-null  object\n",
      " 108  f103    762461 non-null  object\n",
      " 109  f104    475796 non-null  object\n",
      " 110  f105    757830 non-null  object\n",
      " 111  f106    752767 non-null  object\n",
      " 112  f107    684325 non-null  object\n",
      " 113  f108    753236 non-null  object\n",
      " 114  f109    758490 non-null  object\n",
      " 115  f110    526627 non-null  object\n",
      " 116  f111    716778 non-null  object\n",
      " 117  f112    0 non-null       object\n",
      " 118  f113    762461 non-null  object\n",
      " 119  f114    47373 non-null   object\n",
      " 120  f115    281138 non-null  object\n",
      " 121  f116    189460 non-null  object\n",
      " 122  f117    65759 non-null   object\n",
      " 123  f118    40509 non-null   object\n",
      " 124  f119    159478 non-null  object\n",
      " 125  f120    253 non-null     object\n",
      " 126  f121    78341 non-null   object\n",
      " 127  f122    0 non-null       object\n",
      " 128  f123    733350 non-null  object\n",
      " 129  f124    733350 non-null  object\n",
      " 130  f125    733350 non-null  object\n",
      " 131  f126    733350 non-null  object\n",
      " 132  f127    733350 non-null  object\n",
      " 133  f128    733350 non-null  object\n",
      " 134  f129    733350 non-null  object\n",
      " 135  f130    673828 non-null  object\n",
      " 136  f131    651602 non-null  object\n",
      " 137  f132    732066 non-null  object\n",
      " 138  f133    661040 non-null  object\n",
      " 139  f134    688709 non-null  object\n",
      " 140  f135    0 non-null       object\n",
      " 141  f136    0 non-null       object\n",
      " 142  f137    733320 non-null  object\n",
      " 143  f138    695141 non-null  object\n",
      " 144  f139    540432 non-null  object\n",
      " 145  f140    552609 non-null  object\n",
      " 146  f141    552609 non-null  object\n",
      " 147  f142    552609 non-null  object\n",
      " 148  f143    552609 non-null  object\n",
      " 149  f144    552609 non-null  object\n",
      " 150  f145    552609 non-null  object\n",
      " 151  f146    733350 non-null  object\n",
      " 152  f147    733350 non-null  object\n",
      " 153  f148    733350 non-null  object\n",
      " 154  f149    733350 non-null  object\n",
      " 155  f150    695141 non-null  object\n",
      " 156  f151    610008 non-null  object\n",
      " 157  f152    729450 non-null  object\n",
      " 158  f153    729450 non-null  object\n",
      " 159  f154    18468 non-null   object\n",
      " 160  f155    729450 non-null  object\n",
      " 161  f156    729450 non-null  object\n",
      " 162  f157    729450 non-null  object\n",
      " 163  f158    729450 non-null  object\n",
      " 164  f159    729450 non-null  object\n",
      " 165  f160    729450 non-null  object\n",
      " 166  f161    729450 non-null  object\n",
      " 167  f162    729450 non-null  object\n",
      " 168  f163    729450 non-null  object\n",
      " 169  f164    729450 non-null  object\n",
      " 170  f165    729450 non-null  object\n",
      " 171  f166    729450 non-null  object\n",
      " 172  f167    729450 non-null  object\n",
      " 173  f168    729450 non-null  object\n",
      " 174  f169    729450 non-null  object\n",
      " 175  f170    729450 non-null  object\n",
      " 176  f171    729450 non-null  object\n",
      " 177  f172    729450 non-null  object\n",
      " 178  f173    729450 non-null  object\n",
      " 179  f174    729450 non-null  object\n",
      " 180  f175    729450 non-null  object\n",
      " 181  f176    18468 non-null   object\n",
      " 182  f177    729450 non-null  object\n",
      " 183  f178    729450 non-null  object\n",
      " 184  f179    729450 non-null  object\n",
      " 185  f180    729450 non-null  object\n",
      " 186  f181    729450 non-null  object\n",
      " 187  f182    729450 non-null  object\n",
      " 188  f183    729450 non-null  object\n",
      " 189  f184    729450 non-null  object\n",
      " 190  f185    729450 non-null  object\n",
      " 191  f186    729450 non-null  object\n",
      " 192  f187    227395 non-null  object\n",
      " 193  f188    227395 non-null  object\n",
      " 194  f189    4907 non-null    object\n",
      " 195  f190    227395 non-null  object\n",
      " 196  f191    227395 non-null  object\n",
      " 197  f192    227395 non-null  object\n",
      " 198  f193    227395 non-null  object\n",
      " 199  f194    227395 non-null  object\n",
      " 200  f195    227395 non-null  object\n",
      " 201  f196    227395 non-null  object\n",
      " 202  f197    227395 non-null  object\n",
      " 203  f198    729006 non-null  object\n",
      " 204  f199    762521 non-null  object\n",
      " 205  f200    762521 non-null  object\n",
      " 206  f201    762521 non-null  object\n",
      " 207  f202    762521 non-null  object\n",
      " 208  f203    762521 non-null  object\n",
      " 209  f204    762521 non-null  object\n",
      " 210  f205    17033 non-null   object\n",
      " 211  f206    277930 non-null  object\n",
      " 212  f207    277930 non-null  object\n",
      " 213  f208    347508 non-null  object\n",
      " 214  f209    347508 non-null  object\n",
      " 215  f210    347508 non-null  object\n",
      " 216  f211    347508 non-null  object\n",
      " 217  f212    347508 non-null  object\n",
      " 218  f213    701343 non-null  object\n",
      " 219  f214    701343 non-null  object\n",
      " 220  f215    701343 non-null  object\n",
      " 221  f216    699138 non-null  object\n",
      " 222  f217    496013 non-null  object\n",
      " 223  f218    190668 non-null  object\n",
      " 224  f219    488874 non-null  object\n",
      " 225  f220    24092 non-null   object\n",
      " 226  f221    7758 non-null    object\n",
      " 227  f222    486589 non-null  object\n",
      " 228  f223    770146 non-null  object\n",
      " 229  f224    770146 non-null  object\n",
      " 230  f225    770146 non-null  object\n",
      " 231  f226    770146 non-null  object\n",
      " 232  f227    770146 non-null  object\n",
      " 233  f228    770146 non-null  object\n",
      " 234  f229    770146 non-null  object\n",
      " 235  f230    770146 non-null  object\n",
      " 236  f231    770146 non-null  object\n",
      " 237  f232    770146 non-null  object\n",
      " 238  f233    770146 non-null  object\n",
      " 239  f234    770146 non-null  object\n",
      " 240  f235    770146 non-null  object\n",
      " 241  f236    770146 non-null  object\n",
      " 242  f237    770146 non-null  object\n",
      " 243  f238    770146 non-null  object\n",
      " 244  f239    770146 non-null  object\n",
      " 245  f240    770146 non-null  object\n",
      " 246  f241    770146 non-null  object\n",
      " 247  f242    770146 non-null  object\n",
      " 248  f243    770146 non-null  object\n",
      " 249  f244    770146 non-null  object\n",
      " 250  f245    770146 non-null  object\n",
      " 251  f246    770146 non-null  object\n",
      " 252  f247    770146 non-null  object\n",
      " 253  f248    770146 non-null  object\n",
      " 254  f249    770146 non-null  object\n",
      " 255  f250    770146 non-null  object\n",
      " 256  f251    770146 non-null  object\n",
      " 257  f252    770146 non-null  object\n",
      " 258  f253    770146 non-null  object\n",
      " 259  f254    770146 non-null  object\n",
      " 260  f255    770146 non-null  object\n",
      " 261  f256    770146 non-null  object\n",
      " 262  f257    770146 non-null  object\n",
      " 263  f258    770146 non-null  object\n",
      " 264  f259    770146 non-null  object\n",
      " 265  f260    770146 non-null  object\n",
      " 266  f261    770146 non-null  object\n",
      " 267  f262    770146 non-null  object\n",
      " 268  f263    770146 non-null  object\n",
      " 269  f264    770146 non-null  object\n",
      " 270  f265    770146 non-null  object\n",
      " 271  f266    770146 non-null  object\n",
      " 272  f267    770146 non-null  object\n",
      " 273  f268    770146 non-null  object\n",
      " 274  f269    770146 non-null  object\n",
      " 275  f270    770146 non-null  object\n",
      " 276  f271    770146 non-null  object\n",
      " 277  f272    770146 non-null  object\n",
      " 278  f273    770146 non-null  object\n",
      " 279  f274    770146 non-null  object\n",
      " 280  f275    770146 non-null  object\n",
      " 281  f276    770146 non-null  object\n",
      " 282  f277    770146 non-null  object\n",
      " 283  f278    770146 non-null  object\n",
      " 284  f279    770146 non-null  object\n",
      " 285  f280    770146 non-null  object\n",
      " 286  f281    770146 non-null  object\n",
      " 287  f282    770146 non-null  object\n",
      " 288  f283    770146 non-null  object\n",
      " 289  f284    770146 non-null  object\n",
      " 290  f285    770146 non-null  object\n",
      " 291  f286    770146 non-null  object\n",
      " 292  f287    770146 non-null  object\n",
      " 293  f288    770146 non-null  object\n",
      " 294  f289    770146 non-null  object\n",
      " 295  f290    770146 non-null  object\n",
      " 296  f291    770146 non-null  object\n",
      " 297  f292    770146 non-null  object\n",
      " 298  f293    770146 non-null  object\n",
      " 299  f294    770146 non-null  object\n",
      " 300  f295    770146 non-null  object\n",
      " 301  f296    770146 non-null  object\n",
      " 302  f297    770146 non-null  object\n",
      " 303  f298    770146 non-null  object\n",
      " 304  f299    770146 non-null  object\n",
      " 305  f300    770146 non-null  object\n",
      " 306  f301    770146 non-null  object\n",
      " 307  f302    770146 non-null  object\n",
      " 308  f303    770146 non-null  object\n",
      " 309  f304    770146 non-null  object\n",
      " 310  f305    770146 non-null  object\n",
      " 311  f306    770146 non-null  object\n",
      " 312  f307    770146 non-null  object\n",
      " 313  f308    770146 non-null  object\n",
      " 314  f309    770146 non-null  object\n",
      " 315  f310    574423 non-null  object\n",
      " 316  f311    574427 non-null  object\n",
      " 317  f312    574427 non-null  object\n",
      " 318  f313    577761 non-null  object\n",
      " 319  f314    587734 non-null  object\n",
      " 320  f315    587734 non-null  object\n",
      " 321  f316    587734 non-null  object\n",
      " 322  f317    587734 non-null  object\n",
      " 323  f318    587734 non-null  object\n",
      " 324  f319    587734 non-null  object\n",
      " 325  f320    587734 non-null  object\n",
      " 326  f321    587734 non-null  object\n",
      " 327  f322    587734 non-null  object\n",
      " 328  f323    587734 non-null  object\n",
      " 329  f324    587734 non-null  object\n",
      " 330  f325    574411 non-null  object\n",
      " 331  f326    574426 non-null  object\n",
      " 332  f327    574427 non-null  object\n",
      " 333  f328    574427 non-null  object\n",
      " 334  f329    574411 non-null  object\n",
      " 335  f330    574426 non-null  object\n",
      " 336  f331    574427 non-null  object\n",
      " 337  f332    770146 non-null  object\n",
      " 338  f333    770146 non-null  object\n",
      " 339  f334    770146 non-null  object\n",
      " 340  f335    770146 non-null  object\n",
      " 341  f336    638061 non-null  object\n",
      " 342  f337    639264 non-null  object\n",
      " 343  f338    640243 non-null  object\n",
      " 344  f339    640270 non-null  object\n",
      " 345  f340    640270 non-null  object\n",
      " 346  f341    637862 non-null  object\n",
      " 347  f342    638909 non-null  object\n",
      " 348  f343    639888 non-null  object\n",
      " 349  f344    640284 non-null  object\n",
      " 350  f345    640284 non-null  object\n",
      " 351  f346    640284 non-null  object\n",
      " 352  f347    573135 non-null  object\n",
      " 353  f348    573135 non-null  object\n",
      " 354  f349    770164 non-null  object\n",
      " 355  f350    770164 non-null  object\n",
      " 356  f351    628173 non-null  object\n",
      " 357  f352    628173 non-null  object\n",
      " 358  f353    628173 non-null  object\n",
      " 359  f354    628173 non-null  object\n",
      " 360  f355    587734 non-null  object\n",
      " 361  f356    587734 non-null  object\n",
      " 362  f357    587734 non-null  object\n",
      " 363  f358    628173 non-null  object\n",
      " 364  f359    727175 non-null  object\n",
      " 365  f360    232 non-null     object\n",
      " 366  f361    654412 non-null  object\n",
      " 367  f362    654412 non-null  object\n",
      " 368  f363    654412 non-null  object\n",
      " 369  f364    661850 non-null  object\n",
      " 370  f365    661850 non-null  object\n",
      " 371  f366    661850 non-null  object\n",
      "dtypes: object(372)\n",
      "memory usage: 2.1+ GB\n"
     ]
    }
   ],
   "source": [
    "train.info(verbose=True,show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting unnecessary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:17:37.990028Z",
     "iopub.status.busy": "2025-07-05T17:17:37.989714Z",
     "iopub.status.idle": "2025-07-05T17:17:57.031559Z",
     "shell.execute_reply": "2025-07-05T17:17:57.029733Z",
     "shell.execute_reply.started": "2025-07-05T17:17:37.989995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770164, 340)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Drop features with >95% missing\n",
    "missing_pct = train.isna().mean()\n",
    "drop_cols = missing_pct[missing_pct > 0.95].index\n",
    "train.drop(columns=drop_cols, inplace=True)\n",
    "test.drop(columns=[c for c in drop_cols if c in test.columns], inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping cols: 100%|██████████| 340/340 [00:28<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 47 columns\n",
      "train shape: (770164, 293)\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = []\n",
    "for col in tqdm(train.columns,desc=\"Dropping cols: \" ):\n",
    "    unique_vals = train[col].dropna().unique()\n",
    "    nunique = train[col].nunique(dropna=True)\n",
    "    if nunique == 1 or set(unique_vals).issubset({0.0}):\n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "train.drop(columns=cols_to_drop, inplace=True)\n",
    "test.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} columns\")\n",
    "print(f\"train shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:16:10.177762Z",
     "iopub.status.busy": "2025-07-05T17:16:10.177320Z",
     "iopub.status.idle": "2025-07-05T17:17:21.981532Z",
     "shell.execute_reply": "2025-07-05T17:17:21.980456Z",
     "shell.execute_reply.started": "2025-07-05T17:16:10.177724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Standardize missing markers\n",
    "for df in [train, test, events, trans, offers]:\n",
    "    df.replace({-9999.0: pd.NA, None: pd.NA}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting datatypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols : 12 num_cols : 229 ohe_cols : 48\n"
     ]
    }
   ],
   "source": [
    "# 1) Lists from the data dictionary\n",
    "cat_cols = dict_df.loc[dict_df['Type'] == 'Categorical','masked_column'].tolist()\n",
    "num_cols = dict_df.loc[dict_df['Type'] == 'Numerical','masked_column'].tolist()\n",
    "ohe_cols = dict_df.loc[dict_df['Type'] == 'One hot encoded','masked_column'].tolist()\n",
    "\n",
    "# 2) Keep only columns that really exist in *train*\n",
    "cat_cols = [c for c in cat_cols if c in train.columns]\n",
    "num_cols = [c for c in num_cols if c in train.columns]\n",
    "ohe_cols = [c for c in ohe_cols if c in train.columns]\n",
    "print(f\"cat_cols : {len(cat_cols)} num_cols : {len(num_cols)} ohe_cols : {len(ohe_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols.remove('id5')\n",
    "num_cols.remove('id4')\n",
    "cat_cols.remove('id3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:17:21.983290Z",
     "iopub.status.busy": "2025-07-05T17:17:21.982871Z",
     "iopub.status.idle": "2025-07-05T17:17:37.988571Z",
     "shell.execute_reply": "2025-07-05T17:17:37.987663Z",
     "shell.execute_reply.started": "2025-07-05T17:17:21.983264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['id4'] = pd.to_datetime(df['id4'])\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce').dt.date\n",
    "offers['id12'] = pd.to_datetime(offers['id12'])\n",
    "offers['id13'] = pd.to_datetime(offers['id13'])\n",
    "events['id4'] = pd.to_datetime(events['id4'])\n",
    "events['id7'] = pd.to_datetime(events['id7'])\n",
    "trans['f370'] = pd.to_datetime(trans['f370'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f227's unique values : ['0.0' '1.0' <NA>]\n",
      "f228's unique values : ['0.0' '1.0' <NA>]\n",
      "f230's unique values : ['0.0' '1.0' <NA>]\n",
      "f231's unique values : ['1.0' '0.0' <NA>]\n",
      "f232's unique values : ['0.0' '1.0' <NA>]\n",
      "f233's unique values : ['0.0' '1.0' <NA>]\n",
      "f234's unique values : ['0.0' '1.0' <NA>]\n",
      "f235's unique values : ['0.0' '1.0' <NA>]\n",
      "f237's unique values : ['0.0' '1.0' <NA>]\n",
      "f239's unique values : ['0.0' '1.0' <NA>]\n",
      "f241's unique values : ['0.0' '1.0' <NA>]\n",
      "f242's unique values : ['0.0' '1.0' <NA>]\n",
      "f244's unique values : ['0.0' '1.0' <NA>]\n",
      "f247's unique values : ['0.0' '1.0' <NA>]\n",
      "f250's unique values : ['0.0' '1.0' <NA>]\n",
      "f251's unique values : ['0.0' <NA> '1.0']\n",
      "f252's unique values : ['0.0' '1.0' <NA>]\n",
      "f253's unique values : ['0.0' '1.0' <NA>]\n",
      "f254's unique values : ['0.0' '1.0' <NA>]\n",
      "f255's unique values : ['0.0' '1.0' <NA>]\n",
      "f256's unique values : ['0.0' '1.0' <NA>]\n",
      "f257's unique values : ['0.0' '1.0' <NA>]\n",
      "f261's unique values : ['0.0' '1.0' <NA>]\n",
      "f263's unique values : ['0.0' '1.0' <NA>]\n",
      "f264's unique values : ['0.0' '1.0' <NA>]\n",
      "f265's unique values : ['0.0' '1.0' <NA>]\n",
      "f269's unique values : ['0.0' '1.0' <NA>]\n",
      "f272's unique values : ['0.0' '1.0' <NA>]\n",
      "f273's unique values : ['0.0' '1.0' <NA>]\n",
      "f274's unique values : ['0.0' '1.0' <NA>]\n",
      "f275's unique values : ['0.0' '1.0' <NA>]\n",
      "f276's unique values : ['0.0' '1.0' <NA>]\n",
      "f278's unique values : ['0.0' '1.0' <NA>]\n",
      "f280's unique values : ['0.0' '1.0' <NA>]\n",
      "f282's unique values : ['0.0' '1.0' <NA>]\n",
      "f283's unique values : ['0.0' '1.0' <NA>]\n",
      "f284's unique values : ['0.0' '1.0' <NA>]\n",
      "f285's unique values : ['1.0' '0.0' <NA>]\n",
      "f288's unique values : ['0.0' '1.0' <NA>]\n",
      "f289's unique values : ['0.0' '1.0' <NA>]\n",
      "f292's unique values : ['0.0' '1.0' <NA>]\n",
      "f293's unique values : ['0.0' '1.0' <NA>]\n",
      "f296's unique values : ['0.0' '1.0' <NA>]\n",
      "f297's unique values : ['0.0' '1.0' <NA>]\n",
      "f299's unique values : ['0.0' '1.0' <NA>]\n",
      "f302's unique values : ['0.0' '1.0' <NA>]\n",
      "f305's unique values : ['0.0' '1.0' <NA>]\n",
      "f306's unique values : ['0.0' '1.0' <NA>]\n"
     ]
    }
   ],
   "source": [
    "for col in ohe_cols:\n",
    "    print(f\"{col}'s unique values : {train[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting categorical columns: 100%|██████████| 11/11 [00:00<00:00, 12.08it/s]\n",
      "Casting numerical columns: 100%|██████████| 227/227 [01:07<00:00,  3.34it/s]\n",
      "Cleaning and casting one-hot encoded columns: 100%|██████████| 48/48 [00:19<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770164 entries, 0 to 770163\n",
      "Columns: 293 entries, id1 to f366\n",
      "dtypes: Int8(48), category(11), datetime64[ns](1), float32(227), object(6)\n",
      "memory usage: 786.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(cat_cols, desc=\"Casting categorical columns\"):\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "for col in tqdm(num_cols, desc=\"Casting numerical columns\"):\n",
    "    train[col] = pd.to_numeric(train[col], errors='coerce').astype('float32')\n",
    "    test[col] = pd.to_numeric(test[col], errors='coerce').astype('float32')\n",
    "\n",
    "for col in tqdm(ohe_cols, desc=\"Cleaning and casting one-hot encoded columns\"):\n",
    "    train[col] = pd.to_numeric(train[col], errors='coerce').astype('Int8')\n",
    "    test[col] = pd.to_numeric(test[col], errors='coerce').astype('Int8')\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:17:57.033140Z",
     "iopub.status.busy": "2025-07-05T17:17:57.032826Z",
     "iopub.status.idle": "2025-07-05T17:20:11.418994Z",
     "shell.execute_reply": "2025-07-05T17:20:11.417676Z",
     "shell.execute_reply.started": "2025-07-05T17:17:57.033109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineer(df, events, offers):\n",
    "    ctr = events.assign(clicked=events['id7'].notna().astype(int))\n",
    "    df = df.copy()\n",
    "\n",
    "    # unify keys\n",
    "    df['id3'] = df['id3'].astype(str)\n",
    "    ctr['id3'] = ctr['id3'].astype(str)\n",
    "    offers_loc = offers.copy()\n",
    "    offers_loc['id3'] = offers_loc['id3'].astype(str)\n",
    "\n",
    "    # 1) Existing CTR Features (Offer-level only)\n",
    "    o_imp = ctr.groupby('id3').size().rename('imps')\n",
    "    o_click = ctr[ctr.clicked == 1].groupby('id3').size().rename('clicks')\n",
    "    offer_stats = (\n",
    "        pd.concat([o_imp, o_click], axis=1)\n",
    "          .fillna(0)\n",
    "          .assign(ctr=lambda x: x.clicks / x.imps)\n",
    "          .reset_index()\n",
    "    )\n",
    "    df = df.merge(offer_stats[['id3', 'ctr']], on='id3', how='left')\n",
    "\n",
    "    stats = offer_stats.copy()\n",
    "    a, b = 1, 1\n",
    "    stats['ctr_smooth'] = (stats.clicks + a) / (stats.imps + a + b)\n",
    "    df = df.merge(stats[['id3', 'ctr_smooth']], on='id3', how='left')\n",
    "\n",
    "    # 2) Offer Metadata\n",
    "    df = df.merge(\n",
    "        offers_loc[['id3', 'f375', 'f376', 'id12', 'id13', 'id11', 'id10','id8']],\n",
    "        on='id3', how='left'\n",
    "    )\n",
    "\n",
    "    # 3) Popularity in last 30 days (offer-level only)\n",
    "    last_date = df['id4'].max()\n",
    "    cutoff_pop = last_date - pd.Timedelta(days=30)\n",
    "    recent_imps = events[events['id4'] >= cutoff_pop].astype({'id3': str})\n",
    "    total_recent = len(recent_imps)\n",
    "    pop30 = recent_imps.groupby('id3').size().rename('offer_imps_30d')\n",
    "    df['offer_popularity_30d'] = (\n",
    "        df['id3']\n",
    "          .map(pop30.div(total_recent))\n",
    "          .fillna(0)\n",
    "    )\n",
    "\n",
    "    # 4) Temporal & Sequence Features\n",
    "    df['hour'] = df['id4'].dt.hour\n",
    "    df['dow'] = df['id4'].dt.dayofweek\n",
    "    df = df.sort_values(['id2', 'id4'])\n",
    "    df['prev_time'] = df.groupby('id2')['id4'].shift()\n",
    "    df['secs_prev'] = (\n",
    "        df['id4'] - df['prev_time']\n",
    "    ).dt.total_seconds().fillna(-1)\n",
    "    df['days_to_exp'] = (\n",
    "        df['id13'] - df['id4']\n",
    "    ).dt.days.clip(lower=0)\n",
    "\n",
    "    # 5) Frequency & Brand Indicator\n",
    "    df['ind_match'] = 0\n",
    "    df['brand_freq'] = df['id11'].map(\n",
    "        df['id11'].value_counts(normalize=True)\n",
    "    )\n",
    "        # --- 6) Event-driven offer trends ---\n",
    "\n",
    "    # Recent click rate (7-day window)\n",
    "    cutoff_7d = last_date - pd.Timedelta(days=7)\n",
    "    recent_events = events[events['id4'] >= cutoff_7d].copy()\n",
    "    recent_events['id3'] = recent_events['id3'].astype(str)\n",
    "    recent_events['clicked'] = recent_events['id7'].notna().astype(int)\n",
    "\n",
    "    recent_clicks = recent_events.groupby('id3')['clicked'].agg(['sum', 'count']).rename(\n",
    "        columns={'sum': 'recent_clicks', 'count': 'recent_impressions'}\n",
    "    )\n",
    "    recent_clicks['offer_recent_click_rate'] = recent_clicks['recent_clicks'] / recent_clicks['recent_impressions']\n",
    "    df = df.merge(recent_clicks[['offer_recent_click_rate']], on='id3', how='left')\n",
    "\n",
    "    # Median hour of clicks per offer\n",
    "    events['click_hour'] = events['id7'].dt.hour\n",
    "    click_events = events[events['id7'].notna()].copy()\n",
    "\n",
    "    # Total impressions & clicks per offer\n",
    "    click_count = click_events['id3'].value_counts().rename('offer_click_count')\n",
    "    imp_count = events['id3'].value_counts().rename('offer_impression_count')\n",
    "    df = df.merge(imp_count, left_on='id3', right_index=True, how='left')\n",
    "    df = df.merge(click_count, left_on='id3', right_index=True, how='left')\n",
    "\n",
    "    # CTR by offer + day of week\n",
    "    events['dow'] = events['id4'].dt.dayofweek\n",
    "    offer_dow_ctr = (\n",
    "        events.assign(clicked=events['id7'].notna().astype(int))\n",
    "              .groupby(['id3', 'dow'])['clicked']\n",
    "              .agg(['sum', 'count'])\n",
    "              .reset_index()\n",
    "    )\n",
    "    df = df.merge(\n",
    "        offer_dow_ctr[['id3', 'dow']],\n",
    "        on=['id3', 'dow'], how='left'\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_engineer(train, events, offers)\n",
    "test = feature_engineer(test, events, offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['id11']\n",
    "int_cols = ['id2', 'id3', 'y']\n",
    "datetime_cols = ['id5']\n",
    "float_cols = ['f218']\n",
    "\n",
    "for df in [train, test]:\n",
    "    df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "for col in int_cols:\n",
    "    if col in train.columns:\n",
    "        train[col] = pd.to_numeric(train[col], errors='coerce').astype('Int32')\n",
    "    if col in test.columns:\n",
    "        test[col] = pd.to_numeric(test[col], errors='coerce').astype('Int32')\n",
    "\n",
    "for col in datetime_cols:\n",
    "    if col in train.columns:\n",
    "        train[col] = pd.to_datetime(train[col], errors='coerce')\n",
    "    if col in test.columns:\n",
    "        test[col] = pd.to_datetime(test[col], errors='coerce')\n",
    "\n",
    "for col in float_cols:\n",
    "    if col in train.columns:\n",
    "        train[col] = pd.to_numeric(train[col], errors='coerce').astype('float32')\n",
    "    if col in test.columns:\n",
    "        test[col] = pd.to_numeric(test[col], errors='coerce').astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770164 entries, 0 to 770163\n",
      "Columns: 312 entries, id1 to offer_click_count\n",
      "dtypes: Int32(3), Int8(48), category(11), datetime64[ns](5), float32(228), float64(10), int32(2), int64(2), object(3)\n",
      "memory usage: 882.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPREprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = train['id8'].value_counts(normalize=True)\n",
    "train['id8_freq'] = train['id8'].map(freq)\n",
    "test ['id8_freq'] = test ['id8'].map(freq).fillna(0)  # unseen codes → 0\n",
    "\n",
    "# 3) (Optionally) drop the raw id8 if you don’t plan to one‐hot it\n",
    "train.drop(columns=['id8'], inplace=True)\n",
    "test.drop(columns=['id8'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define features\n",
    "num_features = [\n",
    "    'ctr', 'ctr_smooth', 'secs_prev', 'days_to_exp',\n",
    "    'brand_freq', 'offer_popularity_30d',\n",
    "    'offer_recent_click_rate',\n",
    "    'offer_impression_count', 'offer_click_count', 'id8_freq'\n",
    "]\n",
    "cat_ord = ['id10', 'f42', 'f48', 'f53', 'f349']\n",
    "cat_ohe = ['f375', 'f376', 'hour', 'dow',\n",
    "            'f50', 'f52', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "num_features += num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "X = train.copy()\n",
    "X_test = test.copy()\n",
    "\n",
    "# --- Handle categorical columns ---\n",
    "for col in cat_ohe + cat_ord:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "# --- Impute and scale numerical features ---\n",
    "for col in num_features:\n",
    "    median = X[col].median()\n",
    "    X[col] = X[col].fillna(median)\n",
    "    X_test[col] = X_test[col].fillna(median)\n",
    "\n",
    "    mean = X[col].mean()\n",
    "    std = X[col].std()\n",
    "    if std == 0: std = 1  # avoid division by zero\n",
    "\n",
    "    X[col] = (X[col] - mean) / std\n",
    "    X_test[col] = (X_test[col] - mean) / std\n",
    "\n",
    "# --- One-hot encode categorical features ---\n",
    "X_ohe = pd.get_dummies(X[cat_ohe], dummy_na=False)\n",
    "X_test_ohe = pd.get_dummies(X_test[cat_ohe], dummy_na=False)\n",
    "\n",
    "# Align columns to ensure consistency between train and test\n",
    "X_ohe, X_test_ohe = X_ohe.align(X_test_ohe, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "# --- Ordinal encode categorical features ---\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ord_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    "X_ord = pd.DataFrame(\n",
    "    ord_encoder.fit_transform(X[cat_ord]),\n",
    "    columns=cat_ord,\n",
    "    index=X.index\n",
    ")\n",
    "X_test_ord = pd.DataFrame(\n",
    "    ord_encoder.transform(X_test[cat_ord]),\n",
    "    columns=cat_ord,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# --- Already one-hot-encoded columns: Impute NaNs only ---\n",
    "for col in ohe_cols:\n",
    "    X[col] = X[col].fillna(0)\n",
    "    X_test[col] = X_test[col].fillna(0)\n",
    "\n",
    "# --- Final concatenation ---\n",
    "X = pd.concat([X[num_features], X_ohe, X_ord, X[ohe_cols]], axis=1)\n",
    "X_test = pd.concat([X_test[num_features], X_test_ohe, X_test_ord, X_test[ohe_cols]], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:23:26.669495Z",
     "iopub.status.busy": "2025-07-05T17:23:26.669230Z",
     "iopub.status.idle": "2025-07-05T17:27:03.770850Z",
     "shell.execute_reply": "2025-07-05T17:27:03.769719Z",
     "shell.execute_reply.started": "2025-07-05T17:23:26.669473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. Pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "ohe_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=np.nan)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # return array\n",
    "])\n",
    "ord_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=np.nan)),\n",
    "    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan))\n",
    "])\n",
    "alreadyohe_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=np.nan))\n",
    "])\n",
    "\n",
    "# 4. Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('ohe', ohe_pipeline, cat_ohe),\n",
    "    ('ord', ord_pipeline, cat_ord),\n",
    "    ('pass', alreadyohe_pipeline, ohe_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "# 5. Collect all used features (no duplicates)\n",
    "all_features = []\n",
    "for col in num_features + cat_ohe + cat_ord + ohe_cols:\n",
    "    if col not in all_features:\n",
    "        all_features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting and transforming datasets:   0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:597: UserWarning: Skipping features without any observed values: ['brand_freq']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "Fitting and transforming datasets:  50%|█████     | 1/2 [00:42<00:42, 42.75s/it]c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:597: UserWarning: Skipping features without any observed values: ['brand_freq']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "Fitting and transforming datasets: 100%|██████████| 2/2 [00:49<00:00, 24.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# 6. Fit-transform and transform\n",
    "with tqdm(total=2, desc=\"Fitting and transforming datasets\") as pbar:\n",
    "    X = preprocessor.fit_transform(train[all_features])\n",
    "    pbar.update(1)\n",
    "    X_test = preprocessor.transform(test[all_features])\n",
    "    pbar.update(1)\n",
    "y = train['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:27:03.779659Z",
     "iopub.status.busy": "2025-07-05T17:27:03.779293Z",
     "iopub.status.idle": "2025-07-05T17:27:09.083364Z",
     "shell.execute_reply": "2025-07-05T17:27:09.081950Z",
     "shell.execute_reply.started": "2025-07-05T17:27:03.779611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8. Split and group\n",
    "split_cut = train['id4'].quantile(0.8)\n",
    "mask_val = train['id4'] > split_cut\n",
    "mask_val_np = mask_val.to_numpy()  # Convert to NumPy array for sparse matrix indexing\n",
    "\n",
    "X_trn, y_trn = X[~mask_val_np], y[~mask_val_np]\n",
    "X_val, y_val = X[mask_val_np], y[mask_val_np]\n",
    "\n",
    "id2_trn = train.loc[~mask_val, 'id2'].astype(str)\n",
    "id2_val = train.loc[mask_val, 'id2'].astype(str)\n",
    "# 1. Sort everything by group ID\n",
    "trn_sort_idx = id2_trn.argsort()\n",
    "X_trn = X_trn.iloc[trn_sort_idx]\n",
    "y_trn = y_trn.iloc[trn_sort_idx]\n",
    "id2_trn = id2_trn.iloc[trn_sort_idx]\n",
    "\n",
    "val_sort_idx = id2_val.argsort()\n",
    "X_val = X_val.iloc[val_sort_idx]\n",
    "y_val = y_val.iloc[val_sort_idx]\n",
    "id2_val = id2_val.iloc[val_sort_idx]\n",
    "\n",
    "# 2. Now recompute group sizes on sorted id2\n",
    "group_trn = id2_trn.groupby(id2_trn, sort=False).size().values\n",
    "group_val = id2_val.groupby(id2_val, sort=False).size().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by id2 and assign whole groups randomly to train/val\n",
    "unique_ids = train['id2'].unique()\n",
    "np.random.seed(42)\n",
    "val_ids = set(np.random.choice(unique_ids, size=int(0.2 * len(unique_ids)), replace=False))\n",
    "mask_val = train['id2'].isin(val_ids)\n",
    "mask_val_np = mask_val.to_numpy()\n",
    "\n",
    "X_trn, y_trn = X[~mask_val_np], y[~mask_val_np]\n",
    "X_val, y_val = X[mask_val_np], y[mask_val_np]\n",
    "\n",
    "id2_trn = train.loc[~mask_val, 'id2'].astype(str)\n",
    "id2_val = train.loc[mask_val, 'id2'].astype(str)\n",
    "\n",
    "# Filter out id2s with all-zero labels\n",
    "valid_ids_trn = id2_trn[y_trn > 0].unique()\n",
    "valid_ids_val = id2_val[y_val > 0].unique()\n",
    "\n",
    "mask_trn = id2_trn.isin(valid_ids_trn).to_numpy()\n",
    "mask_val = id2_val.isin(valid_ids_val).to_numpy()\n",
    "\n",
    "X_trn, y_trn = X_trn[mask_trn], y_trn[mask_trn]\n",
    "X_val, y_val = X_val[mask_val], y_val[mask_val]\n",
    "id2_trn = id2_trn.iloc[mask_trn]\n",
    "id2_val = id2_val.iloc[mask_val]\n",
    "\n",
    "trn_sort_idx = id2_trn.argsort()\n",
    "X_trn = X_trn[trn_sort_idx]\n",
    "y_trn = y_trn[trn_sort_idx]\n",
    "id2_trn = id2_trn.iloc[trn_sort_idx]\n",
    "\n",
    "val_sort_idx = id2_val.argsort()\n",
    "X_val = X_val[val_sort_idx]\n",
    "y_val = y_val[val_sort_idx]\n",
    "id2_val = id2_val.iloc[val_sort_idx]\n",
    "\n",
    "group_trn = id2_trn.groupby(id2_trn, sort=False).size().values\n",
    "group_val = id2_val.groupby(id2_val, sort=False).size().values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: max_position\n",
      "[LightGBM] [Warning] eval_at is set=7, ndcg_eval_at=7 will be ignored. Current value: eval_at=7\n",
      "[LightGBM] [Warning] Unknown parameter: max_position\n",
      "[LightGBM] [Warning] eval_at is set=7, ndcg_eval_at=7 will be ignored. Current value: eval_at=7\n",
      "[LightGBM] [Info] Total groups: 38065, total data: 616131\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 48735\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 373\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070 Ti Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 184 dense feature groups (108.12 MB) transferred to GPU in 0.100019 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] Unknown parameter: max_position\n",
      "[LightGBM] [Warning] eval_at is set=7, ndcg_eval_at=7 will be ignored. Current value: eval_at=7\n",
      "[LightGBM] [Info] Total groups: 11010, total data: 154033\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's map@7: 0.960761\tvalid's map@7: 0.960687\n",
      "[100]\ttrain's map@7: 0.962433\tvalid's map@7: 0.960914\n",
      "[150]\ttrain's map@7: 0.963923\tvalid's map@7: 0.961188\n",
      "[200]\ttrain's map@7: 0.965347\tvalid's map@7: 0.96149\n",
      "[250]\ttrain's map@7: 0.966464\tvalid's map@7: 0.961689\n",
      "[300]\ttrain's map@7: 0.967624\tvalid's map@7: 0.961711\n",
      "[350]\ttrain's map@7: 0.968518\tvalid's map@7: 0.961987\n",
      "[400]\ttrain's map@7: 0.96959\tvalid's map@7: 0.962097\n",
      "[450]\ttrain's map@7: 0.970754\tvalid's map@7: 0.962428\n",
      "[500]\ttrain's map@7: 0.971677\tvalid's map@7: 0.962611\n",
      "[550]\ttrain's map@7: 0.97253\tvalid's map@7: 0.962598\n",
      "[600]\ttrain's map@7: 0.973426\tvalid's map@7: 0.962739\n",
      "[650]\ttrain's map@7: 0.974343\tvalid's map@7: 0.962864\n",
      "[700]\ttrain's map@7: 0.975272\tvalid's map@7: 0.962915\n",
      "[750]\ttrain's map@7: 0.976119\tvalid's map@7: 0.962883\n",
      "[800]\ttrain's map@7: 0.976951\tvalid's map@7: 0.962958\n",
      "[850]\ttrain's map@7: 0.97765\tvalid's map@7: 0.962737\n",
      "Early stopping, best iteration is:\n",
      "[767]\ttrain's map@7: 0.976474\tvalid's map@7: 0.963124\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import Dataset, early_stopping, log_evaluation\n",
    "train_lgb = lgb.train \n",
    "# 1. Construct LightGBM Datasets\n",
    "train_set = Dataset(X_trn, label=y_trn, group=group_trn,categorical_feature=cat_ord)\n",
    "val_set   = Dataset(X_val, label=y_val, group=group_val,categorical_feature=cat_ord)\n",
    "\n",
    "# 2. Improved Parameters for MAP@7 ranking\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': ['map'],\n",
    "    'eval_at': [7],\n",
    "    'ndcg_eval_at': [7],\n",
    "    'max_position': 7,\n",
    "\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 63,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_gain_to_split': 0.05,\n",
    "\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': 1,\n",
    "    'device_type' : 'gpu',\n",
    "    'gpu_platform_id': 1,  # <-- set your platform id here\n",
    "    'gpu_device_id':   0 ,\n",
    "}\n",
    "\n",
    "# 3. Train with callbacks for early stopping & logging\n",
    "model = train_lgb(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=2000,                   # allow enough rounds\n",
    "    valid_sets=[train_set, val_set],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=100),\n",
    "        log_evaluation(period=50)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:30:54.623990Z",
     "iopub.status.busy": "2025-07-05T17:30:54.623667Z",
     "iopub.status.idle": "2025-07-05T17:36:05.433037Z",
     "shell.execute_reply": "2025-07-05T17:36:05.431863Z",
     "shell.execute_reply.started": "2025-07-05T17:30:54.623967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total groups: 38065, total data: 616131\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 48732\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 373\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070 Ti Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 184 dense feature groups (108.12 MB) transferred to GPU in 0.101346 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Total groups: 11010, total data: 154033\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's map@7: 0.958791\tvalid's map@7: 0.960113\n",
      "[200]\ttrain's map@7: 0.960329\tvalid's map@7: 0.960665\n",
      "[300]\ttrain's map@7: 0.961998\tvalid's map@7: 0.961035\n",
      "[400]\ttrain's map@7: 0.963595\tvalid's map@7: 0.961384\n",
      "[500]\ttrain's map@7: 0.964985\tvalid's map@7: 0.962233\n",
      "[600]\ttrain's map@7: 0.966366\tvalid's map@7: 0.962475\n",
      "Early stopping, best iteration is:\n",
      "[606]\ttrain's map@7: 0.966436\tvalid's map@7: 0.962534\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# 9. LightGBM ranking (MAP@7) with callbacks for early stopping & logging\n",
    "train_set = lgb.Dataset(X_trn, label=y_trn, group=group_trn)\n",
    "val_set   = lgb.Dataset(X_val, label=y_val, group=group_val)\n",
    "\n",
    "params = {\n",
    "    'objective':       'lambdarank',\n",
    "    'metric':          'map',\n",
    "    'eval_at':         [7],\n",
    "    'learning_rate':   0.01,\n",
    "    'num_leaves':      32,\n",
    "    'feature_fraction':0.8,\n",
    "    'bagging_fraction':0.8,\n",
    "    # 'lambda_l1':       1.0,\n",
    "    'bagging_freq':     5,\n",
    "    'verbosity':       1,\n",
    "    'device_type' : 'gpu',\n",
    "    'gpu_platform_id': 1,  # <-- set your platform id here\n",
    "    'gpu_device_id':   0 ,\n",
    "}\n",
    "\n",
    "model = train_lgb(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_set, val_set],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=50),\n",
    "        log_evaluation(period=100)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:36:05.435325Z",
     "iopub.status.busy": "2025-07-05T17:36:05.434967Z",
     "iopub.status.idle": "2025-07-05T17:36:14.422190Z",
     "shell.execute_reply": "2025-07-05T17:36:14.421019Z",
     "shell.execute_reply.started": "2025-07-05T17:36:05.435292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAP@7: 0.7587825740532433\n"
     ]
    }
   ],
   "source": [
    "def map_at_k(ids, y_true, y_pred, k=7):\n",
    "    dfm = pd.DataFrame({\n",
    "        'id2': ids,\n",
    "        'y': y_true,\n",
    "        'p': y_pred\n",
    "    })\n",
    "    # ensure y is numeric\n",
    "    dfm['y'] = dfm['y'].astype(int)\n",
    "\n",
    "    aps = []\n",
    "    for _, g in dfm.groupby('id2'):\n",
    "        top = g.sort_values('p', ascending=False).head(k)\n",
    "        rel = top['y'].values\n",
    "        if rel.sum() == 0:\n",
    "            continue\n",
    "        hits = 0\n",
    "        score = 0.0\n",
    "        for i, r in enumerate(rel, 1):\n",
    "            if r == 1:\n",
    "                hits += 1\n",
    "                score += hits / i\n",
    "        aps.append(score / min(rel.sum(), k))\n",
    "    return np.mean(aps)\n",
    "\n",
    "# Then:\n",
    "val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "validation_map7 = map_at_k(id2_val, y_val, val_preds)\n",
    "print(\"Validation MAP@7:\", validation_map7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `prev_lgb_model` is your previously trained standalone LightGBM model\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importance(importance_type='gain')\n",
    "feature_names = model.feature_name()\n",
    "\n",
    "# Create DataFrame of feature importances\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Extract top 50 features\n",
    "top50_features = feat_imp_df['feature'].head(50).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostRanker\n",
    "import lightgbm as lgb\n",
    "from lightgbm import Dataset, early_stopping, log_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ohe_used = [col for col in cat_ohe if col in X_trn.columns]\n",
    "ohe_cols_used = [col for col in ohe_cols if col in X_trn.columns]\n",
    "cat_ord_used = [col for col in cat_ord if col in X_trn.columns]\n",
    "num_features_used = [col for col in num_features if col in X_trn.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-Hot Pipeline for XGBoost\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('model', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# CatBoost uses raw categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('model', CatBoostClassifier(\n",
    "        cat_features=cat_ord,\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        verbose=0,\n",
    "        task_type='GPU',\n",
    "        random_state=42,\n",
    "        loss_function='Logloss'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Logistic Regression on scaled numeric features\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # handles NaNs\n",
    "    ('model', LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='saga',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_ord_used:\n",
    "    X_trn[col] = X_trn[col].astype(str)\n",
    "    X_val[col] = X_val[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting base models:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:52:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [04:52:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "Fitting base models:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting base models:  67%|██████▋   | 2/3 [00:20<00:11, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LogReg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:597: UserWarning: Skipping features without any observed values: ['brand_freq']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "Fitting base models: 100%|██████████| 3/3 [14:37<00:00, 292.47s/it]\n",
      "c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [05:06:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:597: UserWarning: Skipping features without any observed values: ['brand_freq']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\91820\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\impute\\_base.py:597: UserWarning: Skipping features without any observed values: ['brand_freq']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "\n",
    "# Train base models\n",
    "base_models = [\n",
    "    (\"XGBoost\", xgb_pipeline, X_trn[cat_ohe_used+ohe_cols_used], y_trn),\n",
    "    (\"CatBoost\", cat_pipeline, X_trn[cat_ord_used], y_trn),\n",
    "    (\"LogReg\", logreg_pipeline, X_trn[num_features_used], y_trn)\n",
    "]\n",
    "for name, pipe, X, y_ in tqdm(base_models, desc=\"Fitting base models\"):\n",
    "    y_ = pd.to_numeric(y_) \n",
    "    print(f\"Fitting {name}...\")\n",
    "    pipe.fit(X, y_)\n",
    "\n",
    "# Generate predictions from base models using correct slices\n",
    "Z_trn = np.column_stack([\n",
    "    xgb_pipeline.predict_proba(X_trn[cat_ohe_used+ohe_cols_used])[:, 1],\n",
    "    cat_pipeline.predict_proba(X_trn[cat_ord_used])[:, 1],\n",
    "    logreg_pipeline.predict_proba(X_trn[num_features_used])[:, 1],\n",
    "])\n",
    "\n",
    "Z_val = np.column_stack([\n",
    "    xgb_pipeline.predict_proba(X_val[cat_ohe_used+ohe_cols_used])[:, 1],\n",
    "    cat_pipeline.predict_proba(X_val[cat_ord_used])[:, 1],\n",
    "    logreg_pipeline.predict_proba(X_val[num_features_used])[:, 1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training meta model\n",
      "[LightGBM] [Info] Total groups: 38065, total data: 616131\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 527\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 3\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3070 Ti Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (2.35 MB) transferred to GPU in 0.005233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Total groups: 11010, total data: 154033\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002776 secs. 0 sparse feature groups\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003804 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002741 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002822 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002633 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003267 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002498 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002624 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002557 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002767 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003167 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002654 secs. 0 sparse feature groups\n",
      "[100]\ttrain's map@7: 0.935196\tvalid's map@7: 0.93871\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002588 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003259 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004267 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002423 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002668 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002516 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002773 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002629 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003218 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003474 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002472 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002672 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002376 secs. 0 sparse feature groups\n",
      "[200]\ttrain's map@7: 0.935997\tvalid's map@7: 0.939152\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002363 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.002528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004213 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004026 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004061 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003830 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003519 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.003997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.004135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005693 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.006157 secs. 0 sparse feature groups\n",
      "[300]\ttrain's map@7: 0.936757\tvalid's map@7: 0.939416\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005857 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005900 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.006010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.006214 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.006005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.006147 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005606 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005547 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005648 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005833 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005643 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 3 dense feature groups (1.88 MB) transferred to GPU in 0.005951 secs. 0 sparse feature groups\n",
      "Early stopping, best iteration is:\n",
      "[338]\ttrain's map@7: 0.936991\tvalid's map@7: 0.939608\n"
     ]
    }
   ],
   "source": [
    "group_trn = id2_trn.groupby(id2_trn, sort=False).size().values\n",
    "group_val = id2_val.groupby(id2_val, sort=False).size().values\n",
    "\n",
    "train_set = lgb.Dataset(Z_trn, label=y_trn, group=group_trn)\n",
    "val_set = lgb.Dataset(Z_val, label=y_val, group=group_val)\n",
    "print(\"Training meta model\")\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'map',\n",
    "    'eval_at': [7],\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 32,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbosity': 1,\n",
    "    'device_type': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_set, val_set],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=50),\n",
    "        log_evaluation(period=100)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 50 features for stacking\n",
    "X_meta_train = X_trn[top50_features].copy()\n",
    "X_meta_val   = X_val[top50_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from base models using correct slices\n",
    "Z_trn = np.column_stack([\n",
    "    xgb_pipeline.predict_proba(X_trn[cat_ohe_used+ohe_cols_used])[:, 1],\n",
    "    cat_pipeline.predict_proba(X_trn[cat_ord_used])[:, 1],\n",
    "    logreg_pipeline.predict_proba(X_trn[num_features_used])[:, 1],\n",
    "])\n",
    "\n",
    "Z_val = np.column_stack([\n",
    "    xgb_pipeline.predict_proba(X_val[cat_ohe_used+ohe_cols_used])[:, 1],\n",
    "    cat_pipeline.predict_proba(X_val[cat_ord_used])[:, 1],\n",
    "    logreg_pipeline.predict_proba(X_val[num_features_used])[:, 1],\n",
    "])\n",
    "\n",
    "group_column = 'id2'\n",
    "group_trn = X_trn[group_column].value_counts(sort=False).sort_index().values\n",
    "group_val = X_val[group_column].value_counts(sort=False).sort_index().values\n",
    "\n",
    "X_meta_train = np.hstack([X_meta_train.values, Z_trn])\n",
    "X_meta_val = np.hstack([X_meta_val.values, Z_val])\n",
    "\n",
    "train_set = lgb.Dataset(X_meta_train, label=y_trn, group=group_trn)\n",
    "val_set = lgb.Dataset(X_meta_val, label=y_val, group=group_val)\n",
    "print(\"Training meta model\")\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'map',\n",
    "    'eval_at': [7],\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 16,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbosity': 1,\n",
    "    'device_type': 'gpu',\n",
    "    'gpu_platform_id': 1,\n",
    "    'gpu_device_id': 0\n",
    "}\n",
    "\n",
    "meta_model = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_set, val_set],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=50),\n",
    "        log_evaluation(period=100)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(ids, y_true, y_pred, k=7):\n",
    "    dfm = pd.DataFrame({\n",
    "        'id2': ids,\n",
    "        'y': y_true,\n",
    "        'p': y_pred\n",
    "    })\n",
    "    # ensure y is numeric\n",
    "    dfm['y'] = dfm['y'].astype(int)\n",
    "\n",
    "    aps = []\n",
    "    for _, g in dfm.groupby('id2'):\n",
    "        top = g.sort_values('p', ascending=False).head(k)\n",
    "        rel = top['y'].values\n",
    "        if rel.sum() == 0:\n",
    "            continue\n",
    "        hits = 0\n",
    "        score = 0.0\n",
    "        for i, r in enumerate(rel, 1):\n",
    "            if r == 1:\n",
    "                hits += 1\n",
    "                score += hits / i\n",
    "        aps.append(score / min(rel.sum(), k))\n",
    "    return np.mean(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAP@7 (meta-model): 0.6759015868142365\n"
     ]
    }
   ],
   "source": [
    "train_preds = model.predict(Z_trn, num_iteration=model.best_iteration)\n",
    "train_map7 = map_at_k(id2_trn, y_trn, train_preds)\n",
    "print(\"Train MAP@7 (meta-model):\", train_map7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T17:38:37.864556Z",
     "iopub.status.busy": "2025-07-05T17:38:37.864234Z",
     "iopub.status.idle": "2025-07-05T17:38:54.881906Z",
     "shell.execute_reply": "2025-07-05T17:38:54.880943Z",
     "shell.execute_reply.started": "2025-07-05T17:38:37.864532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_preds[:5]       : [-0.64043362 -3.93230879 -2.64171247  0.05214257 -3.69812285]\n",
      "prob_preds[:5]      : [0.34514853 0.01922166 0.06650165 0.51303269 0.02417126]\n",
      "raw_preds min/max   : -4.754149325862651 3.587385916268742\n",
      "prob_preds min/max  : 0.008542271764755725 0.9730744754414176\n",
      "Saved sigmoid submission to 'r2_submission_file_sigmoid_new.csv'\n",
      "Reloaded CSV pred min/max: 0.0085422717647557 0.9730744754414176\n",
      "                                             id1      id2      id3  \\\n",
      "0     1000061_9914_16-23_2023-11-05 09:11:35.557  1000061     9914   \n",
      "1    1000061_23690_16-23_2023-11-05 09:11:36.193  1000061    23690   \n",
      "2   1000061_522188_16-23_2023-11-05 09:11:37.242  1000061   522188   \n",
      "3  1000061_5420674_16-23_2023-11-05 09:28:04.153  1000061  5420674   \n",
      "4    1000061_27945_16-23_2023-11-05 09:28:04.157  1000061    27945   \n",
      "\n",
      "          id5      pred  \n",
      "0  2023-11-05  0.345149  \n",
      "1  2023-11-05  0.019222  \n",
      "2  2023-11-05  0.066502  \n",
      "3  2023-11-05  0.513033  \n",
      "4  2023-11-05  0.024171  \n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Get raw scores\n",
    "raw_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# 2) Convert to (0,1) via sigmoid\n",
    "prob_preds = expit(raw_preds)\n",
    "\n",
    "# 3) Sanity‑check in memory\n",
    "print(\"raw_preds[:5]       :\", raw_preds[:5])\n",
    "print(\"prob_preds[:5]      :\", prob_preds[:5])\n",
    "print(\"raw_preds min/max   :\", raw_preds.min(), raw_preds.max())\n",
    "print(\"prob_preds min/max  :\", prob_preds.min(), prob_preds.max())\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'id1':  test['id1'],\n",
    "    'id2':  test['id2'],\n",
    "    'id3':  test['id3'],\n",
    "    'id5':  test['id5'],\n",
    "    'pred': prob_preds      # <<-- make sure this is prob_preds\n",
    "})\n",
    "\n",
    "# 5) Save to CSV\n",
    "csv_path = 'r2_submission_file_sigmoid_new.csv'\n",
    "sub.to_csv(csv_path, index=False)\n",
    "print(f\"Saved sigmoid submission to {csv_path!r}\")\n",
    "\n",
    "# 6) Reload and verify on‑disk values\n",
    "df_check = pd.read_csv(csv_path)\n",
    "df_check.head()\n",
    "print(\"Reloaded CSV pred min/max:\", df_check['pred'].min(), df_check['pred'].max())\n",
    "print(df_check.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-05T17:27:20.915381Z",
     "iopub.status.idle": "2025-07-05T17:27:20.915869Z",
     "shell.execute_reply": "2025-07-05T17:27:20.915681Z",
     "shell.execute_reply.started": "2025-07-05T17:27:20.915657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Manual feature name construction\n",
    "num_names = num_features\n",
    "\n",
    "# Nested pipeline access to OneHotEncoder\n",
    "ohe_pipeline = preprocessor.named_transformers_['ohe']\n",
    "ohe_encoder = ohe_pipeline.named_steps['onehot']\n",
    "ohe_cols = ohe_encoder.get_feature_names_out(cat_ohe)\n",
    "\n",
    "ord_cols = cat_ord\n",
    "all_feat_names = np.concatenate([num_names, ohe_cols, ord_cols])\n",
    "\n",
    "# 2. Get importances from LightGBM\n",
    "importances = model.feature_importance(importance_type='gain')\n",
    "\n",
    "# 3. Truncate feat_names to match importances length\n",
    "feat_names = all_feat_names[:len(importances)]\n",
    "\n",
    "# 4. Build DataFrame\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feat_names,\n",
    "    'importance': importances\n",
    "})\n",
    "\n",
    "# 5. Sort and display top 50\n",
    "top50 = feat_imp.sort_values('importance', ascending=False).tail(50)\n",
    "\n",
    "print(\"Top 50 features by importance:\")\n",
    "for i, (f, imp) in enumerate(zip(top50['feature'], top50['importance']), 1):\n",
    "    print(f\"{i:2d}. {f:30s} {imp:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7780290,
     "sourceId": 12343484,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7788745,
     "sourceId": 12354196,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
